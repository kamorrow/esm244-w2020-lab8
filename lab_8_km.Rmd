---
title: "ESM 244 Lab 8"
author: "Keene Morrow"
date: "2/24/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

library(tidyverse)
library(here)
library(pdftools)
library(tidytext)
library(textdata) 
library(ggwordcloud)

# Note - Before lab:
# Attach tidytext and textdata packages
# Run: get_sentiments(lexicon = "nrc")
# Should be prompted to install lexicon - choose yes!
# Run: get_sentiments(lexicon = "afinn")
# Should be prompted to install lexicon - choose yes!
```

### Read in IPCC Report

Read in 32 page IPCC report.  Gets stored as a data frame with one page per line.
View with view(ipcc_text) in the console

```{r}
ipcc_path <- here("data/ipcc_gw_15.pdf")
ipcc_text <- pdf_text(ipcc_path)

ipcc_p9 <- ipcc_text[9]

ipcc_p9
```


### Some wrangling:

- Split up pages into separate lines (separated by `\n`) using `stringr::str_split()`
- Unnest into regular columns using `tidyr::unnest()`
- Remove leading/trailing white space with `stringr::str_trim()`

```{r}
ipcc_df <- data.frame(ipcc_text) %>% 
  mutate(text_full = str_split(ipcc_text, pattern = '\\r\\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 

ipcc_df1 <- data.frame(ipcc_text) %>% 
  mutate(text_full = str_split(ipcc_text, pattern = '\\r\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full))

ipcc_df2 <- data.frame(ipcc_text) %>% 
  mutate(text_full = str_split(ipcc_text, pattern = '\r\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) 

# Why '\\n' instead of '\n'? Because some symbols (e.g. \, *) need to be called literally with a starting \ to escape the regular expression. For example, \\a for a string actually contains \a. So the string that represents the regular expression '\n' is actually '\\n'.

# the extra \ doesn't seem to be necessary on a PC

# More information: https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html
```

### Get tokens using `unnest_tokens()`

Tokenizing recognizes character strings with no internal strings and give each one its own line

```{r}
ipcc_tokens <- ipcc_df %>%
  unnest_tokens(word, text_full)
```

### Count Words
```{r}
ipcc_wc <- ipcc_tokens %>% # This includes a bunch of numbers.
  count(word)  %>% # count = group_by, count, ungroup
  arrange(-n) # places df in descending order by count

```

### Filter out stop words
Check out `view(stop_words)`
You can always make your own if there are stop words in `stop_words` that you want.

```{r}
ipcc_stop <- ipcc_tokens %>%
  anti_join(stop_words) %>% # removes entries with matches
  dplyr::select(-ipcc_text)
```

Remove numeric pieces
```{r}
ipcc_no_numeric <- ipcc_stop %>%
  dplyr::filter(is.na(as.numeric(word))) # tries to make words numeric, then filters for just the ones that are NA aka are words
```

### Start visualization

Words Cloud...
```{r}
ipcc_top100 <- ipcc_no_numeric %>%
  count(word) %>%
  arrange(-n) %>%
  head(100)

ipcc_cloud <- ggplot(data = ipcc_top100, aes(label = word)) +
  geom_text_wordcloud() +
  theme_minimal()

ipcc_cloud
```

With more pretty stuff
```{r}
ggplot(data = ipcc_top100, aes(label = word, size = n)) +
  geom_text_wordcloud_area(aes(color = n), shape = "diamond") +
  scale_size_area(max_size = 12) +
  scale_color_gradientn(colors = c("dark green", "blue", "purple")) +
  theme_minimal()
```

### Sentiment Analysis for Text

```{r}
get_sentiments(lexicon = "afinn")

# look at positive words
afinn_pos <- get_sentiments(lexicon = "afinn") %>%
  filter(value %in% c(4, 5))

get_sentiments(lexicon = "bing")
get_sentiments(lexicon = "nrc")
```

Bind words in `ipcc_stop` to lexicon

```{r}
ipcc_afinn <- ipcc_stop %>%
  inner_join(get_sentiments(lexicon = "afinn"))   # only keeps observations with a match across two data frames
```

*Analysis Idea:* Compare sentiments of different translations of same text (Bweowulf, Song of Roland, Kalevala, Quran, Bible, whatver) 

Let's find some counts (by sentiment ranking):
```{r}
ipcc_afinn_hist <- ipcc_afinn %>% 
  count(value)

ipcc_afinn_hist

# Plot them: 
ggplot(data = ipcc_afinn_hist, aes(x = value, y = n)) +
  geom_col()
```

Investigate some of the words in a bit more depth:
```{r}
# What are these '2' words?
ipcc_afinn2 <- ipcc_afinn %>% 
  filter(value == 2)
```

```{r}
# Check the unique 2-score words:
unique(ipcc_afinn2$word)

# Count & plot them
ipcc_afinn2_n <- ipcc_afinn2 %>% 
  count(word, sort = TRUE)

ggplot(data = ipcc_afinn2_n, aes(x = word, y = n)) +
  geom_col() +
  coord_flip()

# OK so what's the deal with confidence? And is it really "positive" in the emotion sense? 
```

summarize sentiment for the report: 
```{r}
ipcc_summary <- ipcc_afinn %>% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )
```

### NRC lexicon for sentiment analysis

We can use the NRC lexicon to start "binning" text by the feelings they're typically associated with. As above, we'll use inner_join() to combine the IPCC non-stopword text with the nrc lexicon: 

```{r}
ipcc_nrc <- ipcc_stop %>% 
  inner_join(get_sentiments("nrc"))
```

Wait, won't that exclude some of the words in our text? YES! We should check which are excluded using `anti_join()`:

```{r}
ipcc_exclude <- ipcc_stop %>% 
  anti_join(get_sentiments("nrc"))

# View(ipcc_exclude)

# Count to find the most excluded:
ipcc_exclude_n <- ipcc_exclude %>% 
  count(word, sort = TRUE)

head(ipcc_exclude_n)
```

Now find some counts: 
```{r}
ipcc_nrc_n <- ipcc_nrc %>% 
  count(sentiment, sort = TRUE)

# And plot them:

ggplot(data = ipcc_nrc_n, aes(x = sentiment, y = n)) +
  geom_col()

# Annoyingly, not in order...
# use as.factor and fct_reorder to specify the order

ipcc_nrc_n2 <- ipcc_nrc %>% 
  count(sentiment, sort = TRUE) %>%
  mutate(sentiment = as.factor(sentiment)) %>%
  mutate(sentiment = fct_reorder(sentiment, -n))

ggplot(data = ipcc_nrc_n2, aes(x = sentiment, y = n)) +
  geom_col()
```

For each sentiment, what are the top 5 most frequent words associated with that bin.

```{r}
ipcc_nrc_n5 <- ipcc_nrc %>%
  count(word, sentiment, sort = TRUE) %>%
  group_by(sentiment) %>%
  top_n(5) %>% # returns top n in group, expects counts in col n, already grouped
  ungroup()


```

`top_n()` is inclusive of ties, hence 53, not 50 entries

```{r}
ipcc_nrc_gg <- ggplot(data = ipcc_nrc_n5,
                      aes(x = reorder(word, n), # reorder in place, no factor needed
                          y = n),
                          fill = sentiment) +
  geom_col(show.legend = FALSE,
           aes(fill = sentiment)) +
  facet_wrap(~sentiment, ncol = 2, scales = "free")

ipcc_nrc_gg
```

Obviously some issues here.
See sadness: in a scientific context, loss, lower, poverty, cross, and limited may not have the implication of sadness

Hence, text analysis is easy and fast.
The post analysis is super important and needs to be carefully considered.
